{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 경사하강법(GD)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법(Gradient Descent)은 쉽게말해, 비용함수(cost function)를 최소화 하기위해 경사를 반복적으로 하강해가면서 파라미터를 조정해 나가는 것이다.  \n",
    "파라미터 벡터 세타에 대해 비용함수의 현재 gradient를 계산하고, 이를 감소하는 방향으로 0이 될때까지 하강한다. 이는 미분값(기울기)이 0이 되는 지점일 것이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](/assets/images/ML/chap3/gd1.png){: .align-center}{: width=\"80%\" height=\"80%\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처 : https://medium.com/@divakar_239/stochastic-vs-batch-gradient-descent-8820568eada1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법에서 가장 중요한 파라미터는 `학습률(learning rate)` 하이퍼 파라미터로, 하강하는 보폭(step)을 의미한다.  \n",
    "이 `learning rate`가 너무 작으면, 시간이 오래 걸리고 굴곡이 있는 비용함수(변곡점이 많은)의 경우 min값을 찾지 못할 수도 있다.  \n",
    "반대로 너무 크게 설정하면, 최솟값을 가로질러 반대 경사로 뛰어넘는 경우가 발생할 수 있다.  \n",
    "\n",
    "\n",
    "또한 알고리즘이 경사의 왼쪽,오른쪽에서 시작하는 경우에도 결과가 다르게 도출될 수 있는 문제점들이 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](/assets/images/ML/chap3/gd2.png){: .align-center}{: width=\"80%\" height=\"80%\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](/assets/images/ML/chap3/gd3.png){: .align-center}{: width=\"80%\" height=\"80%\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 선형회귀를 위한 MSE 비용함수는 볼록함수(convex function)형태여서 하나의 전역최솟값이 존재하고, 연속된 함수이며 기울기가 갑자기 변하지 않기 때문에 경사하강법으로 전역최솟값에 접근할 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 변수들의 스케일이 매우 다를 경우에도 문제가 생긴다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](/assets/images/ML/chap3/gd4.png){: .align-center}{: width=\"80%\" height=\"80%\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오른쪽 그림은 변수1이 변수2보다 스케일이 작은 훈련데이터인 경우이다.  \n",
    "변수1의 스케일이 훨씬 작기 때문에 비용함수를 변화시키기 위해서는 세타1이 더 크게 바뀌어야 하므로, 좌우로 긴 형태가 된다. 이렇게 되면 최솟값에 도달은 하겠지만 매우 시간이 오래걸리게 된다.  \n",
    "따라서 경사하강법을 이용할때는 반드시 모든 변수들이 같은 스케일을 갖도록 스케일링을 해주어야 한다.  \n",
    "이를 위해 사이킷런의 `StandardScaler`을 통해 표준화를 할 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 위 그림은 2차원(변수가 2개)이지만, 파라미터가 많아질수록(변수가 많아질수록) 최적 조합을 찾는게 어려워진다.  \n",
    "> 선형 회귀의 경우는 쉽게 함수의 맨 바닥에 있겠지만.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. 배치 경사하강법  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
