{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랜덤포레스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[결정트리 포스팅](https://yganalyst.github.io/study/ML_chap5/)에서도 언급했듯이, 랜덤포레스트는 일반적으로 [배깅(또는 페이스팅)](https://yganalyst.github.io/study/ML_chap6-2/)을 적용한 결정트리의 앙상블이다.  \n",
    "\n",
    "앞의 포스팅에서 사용했었던 `BaggingClassifier`에 `DecisionTreeClassifier`를 넣어 앙상블을 만드는 대신  \n",
    "결정트리에 최적화되어 있는 `RandomForestClassifier`를 사용할 수 있다(회귀는 `RandomForestRegressor`).  \n",
    "\n",
    "\n",
    "다음 500개 트리로 이루어진 랜덤포레스트 분류기를 훈련시키는 코드를 보자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moon dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RandomForestClassifier`는 예외가 있지만, `DecisionTreeClassifier`와 `BaggingClassifier`의 매개변수를 모두 가지고 있다.  \n",
    "\n",
    "\n",
    "예외  \n",
    "- `splitter` : 항상 `best`  \n",
    "- `presort` : 항상 False  \n",
    "- `max_samples` : 항상 1  \n",
    "- `base_estimator` : 항상 지정된 매개변수를 사용한 결정트리 모델  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤포레스트는 트리의 노드를 분할할 때 전체 변수 중에서 최선의 변수를 찾는 대신, 무작위로 선택한 변수 후보 중에서 최적의 변수를 찾는 식으로 무작위성을 더 주입한다([결정트리](https://yganalyst.github.io/study/ML_chap5/) 참고).  \n",
    "\n",
    "\n",
    "이는 결국 트리를 더욱 다양하게 만들고 편향을 손해 보는 대신 분산을 낮추어 전체적으로 더 훌륭한 모델을 만든다.  \n",
    "\n",
    "\n",
    "다음은 `BaggingClassifier`를 사용해 랜덤포레스트와 유사하게 만든 것이다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter='random',\n",
    "                                                  max_leaf_nodes=16),\n",
    "                           n_estimators=500,\n",
    "                           max_samples=1.0,\n",
    "                           bootstrap=True,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 엑스트라 트리  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
